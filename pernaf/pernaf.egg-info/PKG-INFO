Metadata-Version: 2.1
Name: pernaf
Version: 0.0.14
Summary: An implementation of the Normalized Advantage Function Reinforcement Learning Algorithm with Prioritized Experience Replay
Home-page: https://github.com/MathPhysSim/PER-NAF
Author: Simon Hirlaender
Author-email: simon.hirlaender@cern.ch
License: UNKNOWN
Description: # PER-NAF
        An implementation of the Normalized Advantage Function Reinforcement Learning Algorithm with Prioritized Experience Replay
        
        ## Summary
        * The original paper of this code is: https://arxiv.org/abs/1509.02971
        * The code is mainly based on: https://github.com/carpedm20/NAF-tensorflow/
        * Additionally I added the prioritized experience replay: https://arxiv.org/abs/1511.05952
        * Using the OpenAI baseline implementation: https://github.com/openai/baselines/blob/master/baselines/deepq/replay_buffer.py
        
        Thanks openAI and Kim!
        
        
        ## Some Advices from experience in RL
        
        * Normalize the state and action space as well as the reward is a good practice
        * Visualise as much as possible to get an intuition about the method as possible bugs
        * If it does not make sense it is a bug with very high probability
        
        
        
        > Coding makes happy 
        
        
        
        
        
Platform: UNKNOWN
Classifier: Programming Language :: Python :: 3
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Requires-Python: >=3.6
Description-Content-Type: text/markdown
